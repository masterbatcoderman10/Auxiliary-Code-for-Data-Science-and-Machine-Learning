{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"./amazon_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_text = list(reviews[\"reviewText\"])\n",
    "\n",
    "reviews_text_cleaned = [reviews_text[i] for i in range(len(reviews_text)) if type(reviews_text[i]) != float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Provide a list of strings - the strings can be from different reviews, headlines and documents. This segment extracts all the words in all the text data.\n",
    "Then extracting the unique words from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(reviews_text_cleaned)\n",
    "#This pattern splits a string with puntuation and spaces\n",
    "pattern = r\"[^?.,!:; ]+\"\n",
    "words = re.findall(pattern, all_text)\n",
    "words_cleaned = [word.lower() for word in words]\n",
    "unique_words = sorted(set(words_cleaned))\n",
    "word_dict_index = {}\n",
    "word_count_dict = {}\n",
    "IDF = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_dict_index[word] = i\n",
    "    word_count_dict[word] = 0 \n",
    "    IDF[word] = 0\n",
    "\n",
    " \n",
    "\n",
    "doc_matrix = np.zeros((len(reviews_text_cleaned), len(unique_words)))\n",
    "tf_idf = np.zeros((len(reviews_text_cleaned), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Term Frequency\n",
    "\n",
    "The followging code segment will determine the term frequency for each text data point and update the doc-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,text_doc in enumerate(reviews_text_cleaned):\n",
    "    pattern = r\"[^?.,!:; ]+\"\n",
    "    text_words = re.findall(pattern, text_doc.lower())\n",
    "    for word in text_words:\n",
    "        w = word_dict_index[word]\n",
    "        doc_matrix[i][w] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inverse Document Frequency\n",
    "\n",
    "The following code segment calculates the inverse document frequency for each word in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_cleaned:\n",
    "    word_count_dict[word] += 1\n",
    "\n",
    "\n",
    "for word in words_cleaned:\n",
    "    ttf = word_count_dict[word]\n",
    "    D = len(reviews_text_cleaned)\n",
    "    idf = np.log((D / ttf))\n",
    "    IDF[word] = idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: TF-IDF\n",
    "\n",
    "The following code will create a doc-term matrix with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,text_doc in enumerate(reviews_text_cleaned):\n",
    "    pattern = r\"[^?.,!:; ]+\"\n",
    "    text_words = re.findall(pattern, text_doc.lower())\n",
    "    for word in text_words:\n",
    "        w = word_dict_index[word]\n",
    "        tf_idf[i][w] = IDF[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self, text_data):\n",
    "        self.text_data = text_data\n",
    "        self.words_cleaned = []\n",
    "        self.word_dict_index = {}\n",
    "        self.word_count_dict = {}\n",
    "        self.IDF = {}        \n",
    "        self.pattern = r\"[^?.,!:; ]+\"\n",
    "        self.unique_words = self.clean()\n",
    "        \n",
    "        for i, word in enumerate(self.unique_words):\n",
    "            self.word_dict_index[word] = i\n",
    "            self.word_count_dict[word] = 0 \n",
    "            self.IDF[word] = 0\n",
    "            \n",
    "        self.total_word_count()\n",
    "        self.doc_matrix = np.zeros((len(self.text_data), len(self.unique_words)))\n",
    "        self.tf_idf = np.zeros((len(self.text_data), len(self.unique_words)))\n",
    "\n",
    "    \n",
    "    def clean(self):\n",
    "        all_text = \" \".join(self.text_data)\n",
    "        words = re.findall(self.pattern, all_text)\n",
    "        self.words_cleaned = [word.lower() for word in words]\n",
    "        return sorted(set(self.words_cleaned))\n",
    "    \n",
    "    def total_word_count(self):\n",
    "        for word in self.words_cleaned:\n",
    "            self.word_count_dict[word] += 1\n",
    "        \n",
    "        #print(self.word_count_dict)\n",
    "\n",
    "    \n",
    "    def tf(self):\n",
    "        for i,text_doc in enumerate(self.text_data):\n",
    "            text_words = re.findall(self.pattern, text_doc.lower())\n",
    "            for word in text_words:\n",
    "                w = self.word_dict_index[word]\n",
    "                self.doc_matrix[i][w] += 1\n",
    "    \n",
    "    def idf(self):\n",
    "\n",
    "        for word in self.words_cleaned:\n",
    "            ttf = self.word_count_dict[word]\n",
    "            D = len(self.text_data)\n",
    "            idf = np.log((D / ttf))\n",
    "            self.IDF[word] = idf\n",
    "    \n",
    "    def create_tf_idf(self):\n",
    "        for i,text_doc in enumerate(self.text_data):\n",
    "            text_words = re.findall(self.pattern, text_doc.lower())\n",
    "            for word in text_words:\n",
    "                w = self.word_dict_index[word]\n",
    "                self.tf_idf[i][w] = self.IDF[word]\n",
    "    \n",
    "    def prepare(self):\n",
    "\n",
    "        self.tf()\n",
    "        self.idf()\n",
    "        self.create_tf_idf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TF_IDF(reviews_text_cleaned)\n",
    "TFIDF.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cbd400ed05db1546209cf0ce449d447a50b69d60697d23e48a62ee68cf2bbaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
